<!DOCTYPE html>
<html>
    <head>
        <meta charset="UTF-8">
        <title>Homework 9</title>
        <link rel="stylesheet" type="text/css" href="resources/homework.css">
        <link rel="stylesheet" type="text/css" href="resources/prism.css">
        <script type="text/javascript" src="resources/prism.js"></script>
        <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
        <script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});</script>
        <style>
            div.algorithm {
                width: 90%;
                margin: 1em auto;
            }
        </style>
    </head>
    <body>
        <div class="content language-markup">
            <h1>CIS 521: Homework 9<span class="points" data-value="100"></span></h1>
            <table class="dates">
                <tr>
                    <td>Release Date</td>
                    <td>Tuesday, April 12, 2016</td>
                </tr>
                <tr>
                    <td>Due Date</td>
                    <td>11:59 pm on Thursday, April 21, 2016</td>
                </tr>
            </table>
            <section>
                <h3>Instructions</h3>
                <p>In this assignment, you will gain experience working with binary and multiclass perceptrons.</p>
                <p>A skeleton file <code>homework9.py</code> containing empty definitions for each question has been provided. Since portions of this assignment will be graded automatically, none of the names or function signatures in this file should be modified. However, you are free to introduce additional variables or functions if needed.</p>
                <p>You may import definitions from any standard Python library, and are encouraged to do so in case you find yourself reinventing the wheel.</p>
                <p>You will find that in addition to a problem specification, most programming questions also include a pair of examples from the Python interpreter. These are meant to illustrate typical use cases, and should not be taken as comprehensive test suites.</p>
                <p>You are strongly encouraged to follow the Python style guidelines set forth in <a href="http://legacy.python.org/dev/peps/pep-0008/">PEP 8</a>, which was written in part by the creator of Python. However, your code will not be graded for style.</p>
                <p>Once you have completed the assignment, you should submit your file on Eniac using the following <code>turnin</code> command, where the flags <code>-c</code> and <code>-p</code> stand for "course" and "project", respectively.</p>
                <pre><code>turnin -c cis521 -p hw9 homework9.py</code></pre>
                <p>You may submit as many times as you would like before the deadline, but only the last submission will be saved. To view a detailed listing of the contents of your most recent submission, you can use the following command, where the flag <code>-v</code> stands for "verbose".</p>
                <pre><code>turnin -c cis521 -p hw9 -v</code></pre>
            </section>
            <section class="numbered">
                <h3>Perceptrons<span class="points" data-value="35"></span></h3>
                <p>In this section, you will implement two varieties of the standard perceptron: one which performs binary classification, distinguishing between positive and negative instances, and one which performs multiclass classification, distinguishing between an arbitrary number of labeled groups.</p>
                <p>As in previous assignments, your use of external code should be limited to built-in Python modules, which excludes packages such as NumPy and NLTK.</p>
                <ol>
                    <li>
                        <p><span class="points" data-value="15"></span>A binary perceptron is one of the simplest examples of a linear classifier. Given a set of data points each associated with a positive or negative label, the goal is to learn a vector $\vec{w}$ such that $\vec{w} \cdot \vec{x}_+ > 0$ for positive instances $\vec{x}_+$ and $\vec{w} \cdot \vec{x}_- \le 0$ for negative instances $\vec{x}_-$.</p>
                        <p>One learning algorithm for this problem initializes the weight vector $\vec{w}$ to the zero vector, then loops through the training data for a fixed number of iterations, adjusting the weight vector whenever a sample is misclassified.</p>
                        <div class="algorithm">
                            <p><span class="bold">Input:</span></p>
                            <p>A list $T$ of training examples $(\vec{x}_1, y_1), \cdots, (\vec{x}_n, y_n)$, where $y_i \in \{+, -\}$; the number of passes $N$ to make over the data set.</p>
                            <p><span class="bold">Output:</span></p>
                            <p>The weight vector $\vec{w}$.</p>
                            <p><span class="bold">Procedure:</span></p>
                            <ol>
                                <li>Initialize the weight vector as $\vec{w} \gets 0$.</li>
                                <li><span class="bold">for</span> iteration $= 1$ <span class="bold">to</span> $N$ <span class="bold">do</span></li>
                                <li style="text-indent: 1em"><span class="bold">for</span> each example $(\vec{x}_i, y_i) \in T$ <span class="bold">do</span></li>
                                <li style="text-indent: 2em">Compute the predicted class as $\hat{y}_i = \text{sign}(\vec{w} \cdot \vec{x}_i)$.</li>
                                <li style="text-indent: 2em"><span class="bold">if</span> $\hat{y}_i \ne y_i$ <span class="bold">then</span></li>
                                <li style="text-indent: 3em">Set $\vec{w} \gets \vec{w} + \vec{x}_i$ if $y_i$ is positive, or $\vec{w} \gets \vec{w} - \vec{x}_i$ if $y_i$ is negative.</li>
                                <li style="text-indent: 2em"><span class="bold">end if</span></li>
                                <li style="text-indent: 1em"><span class="bold">end for</span></li>
                                <li><span class="bold">end for</span></li>
                            </ol>
                        </div>
                        <p>Implement the initialization and prediction methods <code class="language-python">__init__(self, examples, iterations)</code> and <code class="language-python">predict(self, x)</code> in the <code class="language-python">BinaryPerceptron</code> class according to the above specification.</p>
                        <p>During initialization, you should train the weight vector $\vec{w}$ on the input data using <code class="language-python">iterations</code> passes over the data set, then store $\vec{w}$ as an internal variable for future use. Each example in the <code class="language-python">examples</code> list will be a $2$-tuple ($\vec{x}$,&nbsp;$y$) of a data point paired with its binary label <code class="language-python">True</code> or <code class="language-python">False</code>.</p>
                        <p>The prediction method should take as input an unlabeled example $\vec{x}$ and compute the predicted label as $\text{sign}(\vec{w} \cdot \vec{x})$, returning <code class="language-python">True</code> if $\vec{w} \cdot \vec{x} > 0$ or <code class="language-python">False</code> if $\vec{w} \cdot \vec{x} \le 0$.</p>
                        <p>In this assignment, we will represent vectors such as $\vec{w}$ and $\vec{x}$ in Python as dictionary mappings from feature names to values. If a feature is absent from a vector, then its value is assumed to be zero. This allows for efficient storage of sparse, high-dimensional vectors, and encourages efficient implementations which consider only non-zero elements when computing dot products. Note that in general, the weight vector $\vec{w}$ will have a non-zero value associated with every feature, whereas individual instances $\vec{x}$ will have only a handful of non-zero values. You are encouraged to keep this asymmetry in mind when writing your code, as it can impact performance if not taken into account.</p>
                        <pre><code class="language-python">
# Define the training and test data
&gt;&gt;&gt; train = [({"x1": 1}, True), ({"x2": 1}, True), ({"x1": -1}, False),
...    ({"x2": -1}, False)]
&gt;&gt;&gt; test = [{"x1": 1}, {"x1": 1, "x2": 1}, {"x1": -1, "x2": 1.5},
...    {"x1": -0.5, "x2": -2}]

# Train the classifier for one iteration
&gt;&gt;&gt; p = BinaryPerceptron(train, 1)

# Make predictions on the test data
&gt;&gt;&gt; [p.predict(x) for x in test]
[True, True, True, False]
</code></pre>
                    </li>
                    <li>
                        <p><span class="points" data-value="20"></span>A multiclass perceptron uses the same linear classification framework as a binary perceptron, but can accommodate an arbitrary number of classes rather than just two. Given a set of data points and associated labels, where the labels are assumed to be drawn from some set $\{\ell_1, \cdots, \ell_m\}$, the goal is to learn a collection of weight vectors $\vec{w}_{\ell_1}, \cdots, \vec{w}_{\ell_m}$ such that $\text{argmax}_{\ell_k} (\vec{w}_{\ell_k} \cdot \vec{x})$ equals the correct label $\ell$ for each input pair $(\vec{x}, \ell)$.</p>
                        <p>The learning algorithm for this problem is similar to the one for the binary case. All weight vectors are first initialized to zero vectors, and then several passes are made over the training data, with the appropriate weight vectors being adjusted whenever a sample is misclassified.</p>
                        <div class="algorithm">
                            <p><span class="bold">Input:</span></p>
                            <p>A list $T$ of training examples $(\vec{x}_1, y_1), \cdots, (\vec{x}_n, y_n)$, where the labels $y_i$ are drawn from the set $\{\ell_1, \cdots, \ell_m\}$; the number of passes $N$ to make over the data set.</p>
                            <p><span class="bold">Output:</span></p>
                            <p>The weight vectors $\vec{w}_{\ell_1}, \cdots, \vec{w}_{\ell_m}$.</p>
                            <p><span class="bold">Procedure:</span></p>
                            <ol>
                                <li>Initialize the weight vectors as $\vec{w}_{\ell_k} \gets 0$ for $k = 1, \cdots, m$.</li>
                                <li><span class="bold">for</span> iteration $= 1$ <span class="bold">to</span> $N$ <span class="bold">do</span></li>
                                <li style="text-indent: 1em"><span class="bold">for</span> each example $(\vec{x}_i, y_i) \in T$ <span class="bold">do</span></li>
                                <li style="text-indent: 2em">Compute the predicted label as $\hat{y}_i = \text{argmax}_{\ell_k} (\vec{w}_{\ell_k} \cdot \vec{x}_i)$.</li>
                                <li style="text-indent: 2em"><span class="bold">if</span> $\hat{y}_i \ne y_i$ <span class="bold">then</span></li>
                                <li style="text-indent: 3em">Increase the score for the correct class by setting $\vec{w}_{y_i} \gets \vec{w}_{y_i} + \vec{x}_i$.</li>
                                <li style="text-indent: 3em">Decrease the score for the predicted class by setting $\vec{w}_{\hat{y}_i} \gets \vec{w}_{\hat{y}_i} - \vec{x}_i$.</li>
                                <li style="text-indent: 2em"><span class="bold">end if</span></li>
                                <li style="text-indent: 1em"><span class="bold">end for</span></li>
                                <li><span class="bold">end for</span></li>
                            </ol>
                        </div>
                        <p>Implement the initialization and prediction methods <code class="language-python">__init__(self, examples, iterations)</code> and <code class="language-python">predict(self, x)</code> in the <code class="language-python">MulticlassPerceptron</code> class according to the above specification.</p>
                        <p>During initialization, you should train the weight vectors $\vec{w}_{\ell_k}$ on the input data using <code class="language-python">iterations</code> passes over the data set, then store them as internal variables for future use. Each example in the <code class="language-python">examples</code> list will be a $2$-tuple ($\vec{x}$,&nbsp;$y$) of a data point paired with its label. You will have to perform a single pass over the data set at the beginning to determine the set of labels which appear in the training examples. Do not make any assumptions about the form taken on by the labels; they might be numbers, strings, or other Python values.</p>
                        <p>The prediction method should take as input an unlabeled example $\vec{x}$ and return the predicted label $\ell = \text{argmax}_{\ell_k}(\vec{w}_{\ell_k} \cdot \vec{x})$.</p>
                        <pre><code class="language-python">
# Define the training data to be the corners and edge midpoints of the unit square
&gt;&gt;&gt; train = [({"x1": 1}, 1), ({"x1": 1, "x2": 1}, 2), ({"x2": 1}, 3),
...   ({"x1": -1, "x2": 1}, 4), ({"x1": -1}, 5), ({"x1": -1, "x2": -1}, 6),
...   ({"x2": -1}, 7), ({"x1": 1, "x2": -1}, 8)]

# Train the classifier for 10 iterations so that it can learn each class
&gt;&gt;&gt; p = MulticlassPerceptron(train, 10)

# Test whether the classifier correctly learned the training data
&gt;&gt;&gt; [p.predict(x) for x, y in train]
[1, 2, 3, 4, 5, 6, 7, 8]
</code></pre>
                    </li>
                </ol>
            </section>
            <section class="numbered">
                <h3>Applications<span class="points" data-value="60"></span></h3>
                <p>In this section, you will use the general-purpose perceptrons implemented above to create classification systems for a number of specific problems. In each case, you will be responsible for creating feature vectors from the raw data, determining which type of perceptron should be used, and deciding how many passes over the training data should be performed. You will likely require some experimentation to achieve good results. The requisite data sets have been provided as Python objects in <code>homework9_data.py</code>, which has been pre-imported under the module name <code class="language-python">data</code> in the skeleton file.</p>
                <ol>
                    <li>
                        <p><span class="points" data-value="10"></span>Ronald Fisher's iris flower data set has been a benchmark for statistical analysis and machine learning since it was first released in 1936. It contains 50 samples from each of three species of the iris flower: iris setosa, iris versicolor, and iris virginica. Each sample consists of four measurements: the length and width of the sepals and petals of the specimen, in centimeters.</p>
                        <p>Your task is to implement the <code class="language-python">__init__(self, data)</code> and <code class="language-python">classify(self, instance)</code> methods in the <code class="language-python">IrisClassifier</code> class, which should perform training and classification on this data set. Example data will be provided as a list of $2$-tuples ($\vec{x}$,&nbsp;$y$), where $\vec{x}$ is a $4$-tuple of real-valued measurements and $y$ is the name of a species. Test instances will be provided in the same format as the $\vec{x}$ components of the training examples.</p>
                        <table class="codeGroup">
                            <tr>
                                <td>
                                    <pre><code class="language-python">
&gt;&gt;&gt; c = IrisClassifier(data.iris)
&gt;&gt;&gt; c.classify((5.1, 3.5, 1.4, 0.2))
'iris-setosa'
</code></pre>
                                </td>
                                <td>
                                    <pre><code class="language-python">
&gt;&gt;&gt; c = IrisClassifier(data.iris)
&gt;&gt;&gt; c.classify((7.0, 3.2, 4.7, 1.4))
'iris-versicolor'
</code></pre>
                                </td>
                            </tr>
                        </table>
                    </li>
                    <li>
                        <p><span class="points" data-value="10"></span>The National Institute of Standards and Technology has released a collection of bitmap images depicting thousands of handwritten digits from different authors. Though originally presented as $32 \times 32$ blocks of binary pixels, the data has been pre-processed by dividing the images into nonoverlapping blocks of $4 \times 4$ pixels and counting the number of activated pixels in each block. This reduces the dimensionality of the data, making it easier to work with, and also provides some robustness against minor distortions. Each processed image is therefore represented by a list of $8 \times 8 = 64$ values between $0$ and $16$ (inclusive), along with a label between $0$ and $9$ corresponding to the digit which was origially written.</p>
                        <p>Your task is to implement the <code class="language-python">__init__(self, data)</code> and <code class="language-python">classify(self, instance)</code> methods in the <code class="language-python">DigitClassifier</code> class, which should perform training and classification on this data set. Example data will be provided as a list of $2$-tuples ($\vec{x}$,&nbsp;$y$), where $\vec{x}$ is a $64$-tuple of pixel counts between $0$ and $16$ and $y$ is the digit represented by the image. Test instances will be provided in the same format as the $\vec{x}$ components of the training examples.</p>
                        <pre><code class="language-python">
&gt;&gt;&gt; c = DigitClassifier(data.digits)
&gt;&gt;&gt; c.classify((0,0,5,13,9,1,0,0,0,0,13,15,10,15,5,0,0,3,15,2,0,11,8,0,0,4,12,0,0,
...  8,8,0,0,5,8,0,0,9,8,0,0,4,11,0,1,12,7,0,0,2,14,5,10,12,0,0,0,0,6,13,10,0,0,0))
0
</code></pre>
                    </li>
                    <li>
                        <p><span class="points" data-value="10"></span>A simple data set of one-dimensional data is given in <code class="language-python">data.bias</code>, where each example consists of a single positive real-valued feature paired with a binary label. Because the binary perceptron discussed in the previous section contains no bias term, a classifier will not be able to directly distinguish between the two classes of points, despite them being linearly separable. To see why, we observe that if the weight vector (consisting of a single component) is positive, then all instances will be labeled as positive, and if the weight vector is negative, then all instances will be labeled as negative. It is therefore necessary to augment the input data with an additional feature in order to allow a constant bias term to be learned.</p>
                        <p>Your task is to implement the <code class="language-python">__init__(self, data)</code> and <code class="language-python">classify(self, instance)</code> methods in the <code class="language-python">BiasClassifier</code> class to perform training and classification on this data set. Example data will be provided as a list of $2$-tuples ($\vec{x}$,&nbsp;$y$) of real numbers paired with binary labels. Test instances will be single numbers, and should be classified as either <code class="language-python">True</code> or <code class="language-python">False</code>. As discussed above, instances will have to be augmented with an additional feature before being fed into a perceptron in order for proper learning and classification to take place.</p>
                        <pre><code class="language-python">
&gt;&gt;&gt; c = BiasClassifier(data.bias)
&gt;&gt;&gt; [c.classify(x) for x in (-1, 0, 0.5, 1.5, 2)]
[False, False, False, True, True]
</code></pre>
                    </li>
                    <li>
                        <p><span class="points" data-value="15"></span>A mystery data set of two-dimensional data is given in <code class="language-python">data.mystery1</code>, where each example consists of a pair of real-valued features and a binary label. As in the previous problem, this data set is not linearly separable on its own, but each instance can be augmented with one or more additional features derived from the two original features so that linear separation is possible in the new higher-dimensional space.</p>
                        <p>Your task is to implement the <code class="language-python">__init__(self, data)</code> and <code class="language-python">classify(self, instance)</code> methods in the <code class="language-python">MysteryClassifier1</code> class to perform training and classification on this data set. Example data will be provided as a list of $2$-tuples ($\vec{x}$,&nbsp;$y$) of pairs of real numbers and their associated binary labels. Test instances will be pairs of real numbers, and should be classified as either <code class="language-python">True</code> or <code class="language-python">False</code>. Instances will have to be augmented with one or more additional features before being fed into a perceptron in order for proper learning and classification to take place. We recommend first visualizing the data using your favorite plotting software in order to understand its structure, which should help make the appropriate transformation(s) more apparent.</p>
                        <pre><code class="language-python">
&gt;&gt;&gt; c = MysteryClassifier1(data.mystery1)
&gt;&gt;&gt; [c.classify(x) for x in ((0, 0), (0, 1), (-1, 0), (1, 2), (-3, -4))]
[False, False, False, True, True]
</code></pre>
                    </li>
                    <li>
                        <p><span class="points" data-value="15"></span>Another mystery data set of three-dimensional data is given in <code class="language-python">data.mystery2</code>, where each example consists of a triple of real-valued features paired with a binary label. As in the previous few problems, this data set is not linearly separable on its own, but each instance can be augmented with one or more additional features so that linear separation is possible in the new higher-dimensional space.</p>
                        <p>Your task is to implement the <code class="language-python">__init__(self, data)</code> and <code class="language-python">classify(self, instance)</code> methods in the <code class="language-python">MysteryClassifier2</code> class to perform training and classification on this data set. Example data will be provided as a list of $2$-tuples ($\vec{x}$,&nbsp;$y$) of triples of real numbers paired with binary labels. Test instances will be triples of real numbers, and should be classified as either <code class="language-python">True</code> or <code class="language-python">False</code>. Instances will again have to be augmented with one or more additional features before being fed into a perceptron in order for proper learning and classification to take place. As before, we recommend first visualizing the data using your favorite plotting software in order to understand its structure, then thinking about what transformation(s) might help separate the two classes of data.</p>
                        <pre><code class="language-python">
&gt;&gt;&gt; c = MysteryClassifier2(data.mystery2)
&gt;&gt;&gt; [c.classify(x) for x in ((1, 1, 1), (-1, -1, -1), (1, 2, -3), (-1, -2, 3))]
[True, False, False, True]
</code></pre>
                    </li>
                </ol>
            </section>
            <section class="numbered">
                <h3>Feedback<span class="points" data-value="5"></span></h3>
                <ol>
                    <li>
                        <p><span class="points" data-value="1"></span>Approximately how long did you spend on this assignment?</p>
                    </li>
                    <li>
                        <p><span class="points" data-value="2"></span>Which aspects of this assignment did you find most challenging? Were there any significant stumbling blocks?</p>
                    </li>
                    <li>
                        <p><span class="points" data-value="2"></span>Which aspects of this assignment did you like? Is there anything you would have changed?</p>
                    </li>
                </ol>
            </section>
        </div>
    </body>
</html>
